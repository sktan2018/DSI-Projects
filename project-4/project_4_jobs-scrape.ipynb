{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sleep:\n",
    "from time import sleep\n",
    "import random\n",
    "def get_html(driver, url = \"https://www.mycareersfuture.sg/search?search=data&sortBy=new_posting_date&page=0\"):\n",
    "    driver.get(url)\n",
    "    \n",
    "    #wait some time\n",
    "    sleep(random.choice(range(5,6)))\n",
    "    html = driver.page_source\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Search by \"data\"\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver.exe\")\n",
    "url_ref = \"https://www.mycareersfuture.sg\"\n",
    "\n",
    "# Create an empty DataFrame\n",
    "summary = pd.DataFrame(columns=['link','location', 'scheme'])\n",
    "\n",
    "file_num = 0\n",
    "min = 0\n",
    "max = 100\n",
    "\n",
    "for pageNum in range(min,max):\n",
    "    load_success = False\n",
    "    file_num =pageNum+1\n",
    "    #'data'\n",
    "    url = \"https://www.mycareersfuture.sg/search?search=data&sortBy=new_posting_date&page={}\".format(pageNum)\n",
    "        \n",
    "    html = get_html(driver, url)\n",
    "\n",
    "    # Beautiful Soup it!\n",
    "    html = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    #Extract the summary\n",
    "    for job in html.find_all(\"div\", {'class':'card relative'}):\n",
    "        load_success = True\n",
    "        try:\n",
    "            link = url_ref+job.a.get('href')\n",
    "        except:\n",
    "            link = np.nan\n",
    "\n",
    "#         try:\n",
    "#             job_title = job.find(\"div\", {'class':'w-100 '}).text\n",
    "#         except:\n",
    "#             job_title=np.nan\n",
    "\n",
    "        try:\n",
    "            location= job.find(\"p\", {'name':'location'}).text\n",
    "        except:\n",
    "            location=np.nan\n",
    "            \n",
    "        try:\n",
    "            scheme= job.find(\"p\", {'name':'schemes_available'}).text\n",
    "        except:\n",
    "            scheme=np.nan            \n",
    "\n",
    "        summary.loc[len(summary)] = [link, location, scheme]    \n",
    "    \n",
    "    #Write to file for every 10 pages\n",
    "    if ((np.mod(file_num,10) == 0 )| (file_num == max)):\n",
    "        print('links{}.csv'.format(file_num))\n",
    "        summary.to_csv('links{}.csv'.format(file_num))\n",
    "\n",
    "    print('Extract Page {}, total records {}: {}'.format(pageNum, summary.shape[0], (\"passed\" if load_success else \"failed\")))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Page 0, total records 20: passed\n",
      "Extract Page 1, total records 40: passed\n",
      "Extract Page 2, total records 60: passed\n",
      "Extract Page 3, total records 80: passed\n",
      "analyticslinks5.csv\n",
      "Extract Page 4, total records 100: passed\n",
      "Extract Page 5, total records 120: passed\n",
      "Extract Page 6, total records 140: passed\n",
      "Extract Page 7, total records 160: passed\n",
      "Extract Page 8, total records 180: passed\n",
      "analyticslinks10.csv\n",
      "Extract Page 9, total records 200: passed\n",
      "Extract Page 10, total records 220: passed\n",
      "Extract Page 11, total records 240: passed\n",
      "Extract Page 12, total records 260: passed\n",
      "Extract Page 13, total records 280: passed\n",
      "analyticslinks15.csv\n",
      "Extract Page 14, total records 300: passed\n",
      "Extract Page 15, total records 320: passed\n",
      "analyticslinks17.csv\n",
      "Extract Page 16, total records 338: passed\n"
     ]
    }
   ],
   "source": [
    "# Search by \"Business Intelligence\"\n",
    "# Search by 'research scientist'\n",
    "# Search by 'database administrator'\n",
    "# Search by 'database developer'\n",
    "# Search by 'statistician'\n",
    "# Search by 'data scientist'\n",
    "# Search by 'data analyst'\n",
    "# Search by 'data engineer'\n",
    "# Search by 'data consultant'\n",
    "# Search by 'big data'\n",
    "# Search by 'hadoop'\n",
    "# Search by 'data analytics'\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver.exe\")\n",
    "url_ref = \"https://www.mycareersfuture.sg\"\n",
    "\n",
    "# Create an empty DataFrame\n",
    "summary = pd.DataFrame(columns=['link','location', 'scheme'])\n",
    "\n",
    "file_num = 0\n",
    "min = 0\n",
    "max = 17\n",
    "\n",
    "for pageNum in range(min,max):\n",
    "    load_success = False\n",
    "    file_num =pageNum+1\n",
    "#     # 'data'\n",
    "#     url = \"https://www.mycareersfuture.sg/search?search=data&sortBy=new_posting_date&page={}\".format(pageNum)\n",
    "    \n",
    "#     #'research scientist'\n",
    "#     url = \"https://www.mycareersfuture.sg/search?search=research%20scientist&sortBy=new_posting_date&page={}\".format(pageNum)\n",
    "\n",
    "#     #'business intelligence'\n",
    "#     url = \"https://www.mycareersfuture.sg/search?search=business%20intelligence&sortBy=new_posting_date&page={}\".format(pageNum)\n",
    "\n",
    "#     #'database administrator'\n",
    "#     url = \"https://www.mycareersfuture.sg/search?search=database%20administrator&sortBy=new_posting_date&page={}\".format(pageNum)\n",
    "    \n",
    "#     #'database developer'\n",
    "#     url = \"https://www.mycareersfuture.sg/search?search=database%20developer&sortBy=new_posting_date&page={}\".format(pageNum)\n",
    "\n",
    "#     # 'statistician'\n",
    "#     url = \"https://www.mycareersfuture.sg/search?search=statistician&sortBy=new_posting_date&page={}\".format(pageNum)\n",
    "\n",
    "#     # 'data scientist'\n",
    "#     url = \"https://www.mycareersfuture.sg/search?search=data%20scientist&sortBy=new_posting_date&page={}\".format(pageNum)\n",
    "\n",
    "#     # 'data analyst'\n",
    "#     url = \"https://www.mycareersfuture.sg/search?search=data%20analyst&sortBy=new_posting_date&page={}\".format(pageNum)\n",
    "      \n",
    "#     # 'data engineer'     \n",
    "#     url = \"https://www.mycareersfuture.sg/search?search=data%20engineer&sortBy=new_posting_date&page={}\".format(pageNum)\n",
    "\n",
    "#     # 'data consultant'     \n",
    "#     url = \"https://www.mycareersfuture.sg/search?search=data%20consultant&sortBy=new_posting_date&page={}\".format(pageNum)\n",
    "\n",
    "#     # 'big data'     \n",
    "#     url = \"https://www.mycareersfuture.sg/search?search=big%20data&sortBy=new_posting_date&page={}\".format(pageNum)\n",
    "\n",
    "#     # 'hadoop'\n",
    "#     url = \"https://www.mycareersfuture.sg/search?search=hadoop&sortBy=new_posting_date&page={}\".format(pageNum)\n",
    "    \n",
    "    # 'data analytics'\n",
    "    url = \"https://www.mycareersfuture.sg/search?search=data%20analytics&sortBy=new_posting_date&page={}\".format(pageNum)\n",
    "    \n",
    "    html = get_html(driver, url)\n",
    "\n",
    "    # Beautiful Soup it!\n",
    "    html = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    #Extract the summary\n",
    "    for job in html.find_all(\"div\", {'class':'card relative'}):\n",
    "        load_success = True\n",
    "        try:\n",
    "            link = url_ref+job.a.get('href')\n",
    "        except:\n",
    "            link = np.nan\n",
    "\n",
    "#         try:\n",
    "#             job_title = job.find(\"div\", {'class':'w-100 '}).text\n",
    "#         except:\n",
    "#             job_title=np.nan\n",
    "\n",
    "        try:\n",
    "            location= job.find(\"p\", {'name':'location'}).text\n",
    "        except:\n",
    "            location=np.nan\n",
    "            \n",
    "        try:\n",
    "            scheme= job.find(\"p\", {'name':'schemes_available'}).text\n",
    "        except:\n",
    "            scheme=np.nan            \n",
    "\n",
    "        summary.loc[len(summary)] = [link, location, scheme]    \n",
    "    \n",
    "    #Write to file for every 10 pages\n",
    "    if ((np.mod(file_num,5) == 0 )| (file_num == max)):\n",
    "        print('analyticslinks{}.csv'.format(file_num))\n",
    "        summary.to_csv('analyticslinks{}.csv'.format(file_num))\n",
    "\n",
    "    print('Extract Page {}, total records {}: {}'.format(pageNum, summary.shape[0], (\"passed\" if load_success else \"failed\")))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Row 300: passed\n",
      "Extract Row 301: passed\n",
      "Extract Row 302: passed\n",
      "Extract Row 303: passed\n",
      "Extract Row 304: passed\n",
      "Extract Row 305: passed\n",
      "Extract Row 306: passed\n",
      "Extract Row 307: passed\n",
      "Extract Row 308: passed\n",
      "Extract Row 309: passed\n",
      "Extract Row 310: passed\n",
      "Extract Row 311: passed\n",
      "Extract Row 312: passed\n",
      "Extract Row 313: passed\n",
      "Extract Row 314: passed\n",
      "Extract Row 315: passed\n",
      "Extract Row 316: passed\n",
      "Extract Row 317: passed\n",
      "Extract Row 318: passed\n",
      "Extract Row 319: passed\n",
      "Extract Row 320: passed\n",
      "Extract Row 321: passed\n",
      "Extract Row 322: passed\n",
      "Extract Row 323: passed\n",
      "Extract Row 324: passed\n",
      "Extract Row 325: passed\n",
      "Extract Row 326: passed\n",
      "Extract Row 327: passed\n",
      "Extract Row 328: passed\n",
      "Extract Row 329: passed\n",
      "Extract Row 330: passed\n",
      "Extract Row 331: passed\n",
      "Extract Row 332: passed\n",
      "Extract Row 333: passed\n",
      "Extract Row 334: passed\n",
      "Extract Row 335: passed\n",
      "Extract Row 336: passed\n",
      "Extract Row 337: passed\n",
      "analyticsjobs338.csv\n"
     ]
    }
   ],
   "source": [
    "# To scape the details for each job\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver.exe\")\n",
    "#grab url,  from the DataFrame\n",
    "link_df = pd.read_csv('./analyticslinks17.csv')\n",
    "\n",
    "# # Create an empty DataFrame\n",
    "jobs = pd.DataFrame(columns=['link','location','scheme', 'company',\n",
    "                             'job_title','address','employment_type', 'seniority',\n",
    "                             'job_categories','salary','sal_type','num_of_applications', \n",
    "                             'last_posted_date', 'expiry_date','company_info','job_desc','job_req'])\n",
    "\n",
    "min = 300\n",
    "max = 340\n",
    "if min < 0 :\n",
    "    min = 0\n",
    "\n",
    "if max > link_df.shape[0]:\n",
    "    max = link_df.shape[0]\n",
    "    \n",
    "for row in range (min, max):\n",
    "    url = link_df.loc[row, 'link']\n",
    "#     job_title = link_df.loc[row, 'job_title']\n",
    "    location = link_df.loc[row, 'location']\n",
    "    scheme = link_df.loc[row, 'scheme']\n",
    "    \n",
    "    html = get_html(driver, url)\n",
    "    \n",
    "    print('Extract Row {}: {}'.format(row, (\"passed\" if (len(html)!= 0) else \"failed\")))   \n",
    "    \n",
    "    # Beautiful Soup it!\n",
    "    html = BeautifulSoup(html, 'lxml')\n",
    "    job = html.find(\"section\", {'id':'job_details'})\n",
    "        \n",
    "    try:\n",
    "        company= job.find('p',{'name':'company'}).text\n",
    "    except:\n",
    "        company=np.nan \n",
    "\n",
    "    try:\n",
    "        job_title= job.find('h1',{'id':'job_title'}).text\n",
    "    except:\n",
    "        job_title=np.nan\n",
    "        \n",
    "    try:\n",
    "        address= job.find('p',{'id':'address'}).text\n",
    "    except:\n",
    "        address=np.nan\n",
    "\n",
    "    try:\n",
    "        employment_type= job.find('p',{'id':'employment_type'}).text\n",
    "    except:\n",
    "        employment_type=np.nan\n",
    "\n",
    "    try:\n",
    "        seniority= job.find('p',{'id':'seniority'}).text\n",
    "    except:\n",
    "        seniority=np.nan\n",
    "\n",
    "    try:\n",
    "        job_categories= job.find('p',{'id':'job-categories'}).text\n",
    "    except:\n",
    "        job_categories=np.nan\n",
    "\n",
    "    # Salary\n",
    "    try:\n",
    "        salary= job.find('span',{'class':'salary_range dib f2-5 fw6 black-80'}).text\n",
    "    except:\n",
    "        salary=np.nan\n",
    "        \n",
    "    try:\n",
    "        sal_type= job.find('span',{'class':'salary_type dib f5 fw4 black-60 pr1 i pb'}).text\n",
    "    except:\n",
    "        sal_type=np.nan\n",
    "\n",
    "    # Application Status\n",
    "    try:\n",
    "        num_of_applications= job.find('span',{'id':'num_of_applications'}).text\n",
    "    except:\n",
    "        num_of_applications=np.nan\n",
    "        \n",
    "    try:\n",
    "        last_posted_date= job.find('span',{'id':'last_posted_date'}).text\n",
    "    except:\n",
    "        last_posted_date=np.nan\n",
    "        \n",
    "    try:\n",
    "        expiry_date= job.find('span',{'id':'expiry_date'}).text\n",
    "    except:\n",
    "        expiry_date=np.nan\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        company_info= job.find('div',{'data-cy':'companyinfo-writeup'}).text\n",
    "    except:\n",
    "        company_info=np.nan\n",
    "    \n",
    "    try:\n",
    "        job_desc= job.find('div',{'id':'description-content'}).text\n",
    "    except:\n",
    "        job_desc=np.nan\n",
    "    \n",
    "    try:\n",
    "        job_req= job.find('div',{'id':'requirements-content'}).text\n",
    "    except:\n",
    "        job_req=np.nan\n",
    "\n",
    "    jobs.loc[len(jobs)] = [url,location,scheme,company,job_title,\n",
    "                          address,employment_type,seniority,job_categories,\n",
    "                          salary,sal_type,num_of_applications,last_posted_date,\n",
    "                          expiry_date,company_info,job_desc,job_req] \n",
    "    \n",
    "#     Write to file for every 100 links\n",
    "    if ((np.mod(row+1,100) == 0 )| (row == max-1)):\n",
    "        print('analyticsjobs{}.csv'.format(row+1))\n",
    "        jobs.to_csv('analyticsjobs{}.csv'.format(row+1),index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = test.split(\"to\")[0]\n",
    "# print(a)\n",
    "# b = test.split(\"to\")[1]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver.exe\")\n",
    "\n",
    "# #grab url,  from the DataFrame\n",
    "# # link_df = pd.read_csv('./links200.csv')\n",
    "\n",
    "# # # Create an empty DataFrame\n",
    "# jobs = pd.DataFrame(columns=['link','location','scheme', 'company',\n",
    "#                              'job_title','address','employment_type', 'seniority',\n",
    "#                              'job_categories','sal_low','sal_type','num_of_applications', \n",
    "#                              'last_posted_date', 'expiry_date','company_info','job_desc','job_req'])\n",
    "\n",
    "# # min = 0\n",
    "# # max = 4000\n",
    "# # if min < 0 :\n",
    "# #     min = 0\n",
    "\n",
    "# # if max > link_df.shape[0]:\n",
    "# #     max = link_df.shape[0]\n",
    "    \n",
    "# for row in range (0, 1):\n",
    "#     url = 'https://www.mycareersfuture.sg/job/new-grad-senior-software-engineer-visa-worldwide-2509f121bd57ddda1314da3373f94f8f'\n",
    "#     location = 'East'\n",
    "#     scheme = np.nan #'Government support available'\n",
    "    \n",
    "#     html = get_html(driver, url)\n",
    "    \n",
    "#     print('Extract Row {}: {}'.format(row, (\"passed\" if (len(html)!= 0) else \"failed\")))   \n",
    "    \n",
    "#     # Beautiful Soup it!\n",
    "#     html = BeautifulSoup(html, 'lxml')\n",
    "#     job = html.find(\"section\", {'id':'job_details'})\n",
    "        \n",
    "#     try:\n",
    "#         company= job.find('p',{'name':'company'}).text\n",
    "#     except:\n",
    "#         company=np.nan \n",
    "\n",
    "#     try:\n",
    "#         job_title= job.find('h1',{'id':'job_title'}).text\n",
    "#     except:\n",
    "#         job_title=np.nan\n",
    "        \n",
    "#     try:\n",
    "#         address= job.find('p',{'id':'address'}).text\n",
    "#     except:\n",
    "#         address=np.nan\n",
    "\n",
    "#     try:\n",
    "#         employment_type= job.find('p',{'id':'employment_type'}).text\n",
    "#     except:\n",
    "#         employment_type=np.nan\n",
    "\n",
    "#     try:\n",
    "#         seniority= job.find('p',{'id':'seniority'}).text\n",
    "#     except:\n",
    "#         seniority=np.nan\n",
    "\n",
    "#     try:\n",
    "#         job_categories= job.find('p',{'id':'job-categories'}).text\n",
    "#     except:\n",
    "#         job_categories=np.nan\n",
    "\n",
    "#     # Salary\n",
    "#     try:\n",
    "#         salary= job.find('span',{'class':'salary_range dib f2-5 fw6 black-80'}).text\n",
    "#     except:\n",
    "#         salary=np.nan\n",
    "        \n",
    "#     try:\n",
    "#         sal_type= job.find('span',{'class':'salary_type dib f5 fw4 black-60 pr1 i pb'}).text\n",
    "#     except:\n",
    "#         sal_type=np.nan\n",
    "\n",
    "#     # Application Status\n",
    "#     try:\n",
    "#         num_of_applications= job.find('span',{'id':'num_of_applications'}).text\n",
    "#     except:\n",
    "#         num_of_applications=np.nan\n",
    "        \n",
    "#     try:\n",
    "#         last_posted_date= job.find('span',{'id':'last_posted_date'}).text\n",
    "#     except:\n",
    "#         last_posted_date=np.nan\n",
    "        \n",
    "#     try:\n",
    "#         expiry_date= job.find('span',{'id':'expiry_date'}).text\n",
    "#     except:\n",
    "#         expiry_date=np.nan\n",
    "        \n",
    "    \n",
    "#     try:\n",
    "#         company_info= job.find('div',{'data-cy':'companyinfo-writeup'}).text\n",
    "#     except:\n",
    "#         company_info=np.nan\n",
    "    \n",
    "#     try:\n",
    "#         job_desc= job.find('div',{'id':'description-content'}).text\n",
    "#     except:\n",
    "#         job_desc=np.nan\n",
    "    \n",
    "#     try:\n",
    "#         job_req= job.find('div',{'id':'requirements-content'}).text\n",
    "#     except:\n",
    "#         job_req=np.nan\n",
    "\n",
    "#     jobs.loc[len(jobs)] = [url,location,scheme,company,job_title,\n",
    "#                           address,employment_type,seniority,job_categories,\n",
    "#                           salary,sal_type,num_of_applications,last_posted_date,\n",
    "#                           expiry_date,company_info,job_desc,job_req] \n",
    "    \n",
    "# # #     Write to file for every 100 links\n",
    "# #     if ((np.mod(row+1,100) == 0 )| (row == max-1)):\n",
    "# #         print('jobs{}.csv'.format(row+1))\n",
    "#     jobs.to_csv('jobs{}.csv'.format(11),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
